\documentclass{article}
\usepackage{xeCJK} % For CJK support
\usepackage[margin=1in]{geometry} % Set all margins to 1 inch
\usepackage{parskip}% Add more space

\usepackage{amsmath} % is needed to use the math environments align, aligned, gather, gathered, multline etc.
\usepackage{amsfonts} % For math fonts like \mathbb{R}
\usepackage{amsthm} % For theorem environments

\usepackage{graphicx}
\graphicspath{ {./Media/} }
\usepackage{caption} % For captions in figures and tables


\usepackage{cases} % For \begin{numcases} environment, included after amsmath
\usepackage{braket} % For \set{} command

\usepackage[hidelinks]{hyperref}
\newcommand{\0}{\mathit{0}}
\newcommand{\V}{\mathsf{V}}


\begin{document}
\section{Page 12}
\textbf{Theorem 1.2}. In any vector space $\V$, the following statements are true:

(a) $0x = \0$ for each $x \in \V$.

(b) $(-a)x = -(ax)=a(-x)$ for each $a \in F$ and each $x \in \V$.

(c) $a\0 = \0$ for each $a \in F$.

The proof of (c) is similar to the proof of (a)

\begin{quotation}
    \textbf{Theorem 1.1} (Cancellation Law for Vector Addition). If $x$, $y$,
    and $z$ are vectors in a vector space $\V$ such that $x+z = y +z$,then$x = y$.

    (\textbf{VS 7}) For each element $a$ in $F$ and each pair of elements $x$, $y$ in $\V$,
    $a(x +y)=ax+ay$. (See page 7)
\end{quotation}

\begin{proof}
    By VS 7, we have $a\0+a\0=a(\0+\0)=a\0=a\0+\0$.
    Then by the cancellation law for vector addition, we have $a\0 = \0$.
\end{proof}

\section{Thoughts on exercise 1.2.1(c)(d)}

\begin{quotation}

    1. Label the following statements as true or false.

    (c)In any vector space, $ax = bx$ implies that $a = b$.

    (d)In any vector space, $ax = ay$ implies that $x = y$.

    (\textbf{VS 5}) For each element $x$ in $\V$, $1x = x$.

    (\textbf{VS 6}) For each pair of elements $a$, $b$ in $F$ and each element $x$ in $\V$,
    $(ab)x = a(bx)$.

\end{quotation}

It's obvious that in (c) and (d) above, $0$ and $\0$ is a counterexample respectively. Then:

If $ax=\0\ (a\in F,x\in\V)$, is it possible that $a\neq0$ and $v\neq\0$?

\begin{proof}[Answer]
    No, it's impossible.

    Since $a$ is an element in field,
    so there's multiplicative inverse $a^{-1}$ such that $a^{-1} a=1$.

    By VS 6 and VS 5, $a^{-1} (ax)=(a^{-1}a)x=1x=x$, also $a^{-1} (ax)=a^{-1}\0=0$.
\end{proof}

\section{Thoughts on Exercise 1.3.28}
Why is the condition "$F$ is not of characteristic $2$" needed?

\begin{quotation}
    28. A matrix $M$ is called \textit{skew-symmetric} if $M^t = -M$.
    Clearly, a skew-symmetric matrix is square. Let $F$ be a field. Prove that the set $\mathsf{W}_1$ of all skew-symmetric $n\times n$ matrices with entries from F is a subspace of $\mathsf{M}_{n\times n}(F)$. Now assume that $F$ is not of characteristic 2 (see Ap
    pendix C), and let $\mathsf{W}_2$ be the subspace of $\mathsf{M}_{n\times n}(F)$ consisting of all
    symmetric $n\times n$ matrices. Prove that$\mathsf{M}_{n\times n}(F)=\mathsf{W}_1 \oplus \mathsf{W}_2$.


    \textbf{Example 4}
    The field $Z_2$ consists of two elements $0$ and $1$ with the operations of addition
    and multiplication defined by the equations
    \begin{gather*}
        0+0=0,\quad 0+1=1+0=1,\quad1+1=0,\\
        0\cdot 0=0, \quad 0\cdot 1=1\cdot 0=0, \quad \text{and} \quad1\cdot 1=1
    \end{gather*}

    In an arbitrary field $F$, it may happen that a sum $1+1+\cdots +1$($p$ sum
    mands) equals $0$ for some positive integer $p$. For example, in the field $Z_2$
    (defined in Example 4), $1+1 = 0$. In this case, the smallest positive integer $p$
    for which a sum of $p$ $1$’s equals $0$ is called the \textbf{characteristic} of $F$; if no such
    positive integer exists, then F is said to have characteristic zero.Thus $Z_2$
    has characteristic two, and $R$ has characteristic zero. Observe that if $F$ is a
    field of characteristic $p=0$,then $x+x+\cdots +x$ ($p$ summands) equals $0$ for all
    $x \in F$.

\end{quotation}

\begin{proof}
    The solution mentioned
    \begin{quotation}
        $\mathsf{W}_1 \cap\mathsf{W}_2 ={\0}$ holds is because $A+A^t$ is symmetric and $A-A^t$ is skew-symmetric. If $F$ is of characteristic 2, we have
        $\mathsf{W}_1  =\mathsf{W}_2 $
    \end{quotation}

    if there's other element $A$ other than $\0$ in $\mathsf{W}_1 \cap\mathsf{W}_2$, then there is non-zero entry $A_{ij}=A_{ji}$ and $A_{ij}=-A_{ji}$, which leads to
    $A_{ij}+A_{ij}=0$.

    Devide both hands of equity by $A_{ij}$ causes $1+1=0$, which means $F$ is of characteristic $2$”
\end{proof}

\section{Thoughts on exercise 1.5.13}
Where is the condition "characteristic not equal to two" used?

\begin{quotation}
    13. Let $\V$ be a vector space over a field of characteristic
    not equal to two.

    (a) Let $u$ and $v$ be distinct vectors in $V$. Prove that $\set{u, v}$ is
    linearly independent if and only if $\set{u + v, u - v}$ is linearly independent.

    (b) Let $u, v,$ and $w$ be distinct vectors in $\V$. Prove that $\set{u, v, w}$
    is linearly independent if and only if $\set{u + v, u + w, v + w}$ is linearly
    independent.
\end{quotation}

Since the proof of (a) and (b) are similar,
we only show where the characteristic not equal to two is used in the sufficiency case of (a).
For the rest, the situation is similar.

\begin{proof}
    If $\set{u+v, u-v}$ is linearly independent, we have
    $m(u+v) + n(u - v) = \0 \implies m = n = \0$.
    Assume $xu+yv=0$, then we try to represent $xu+yv$ as linear combination of $u+v$ and $u-v$.
    Let $a(u+v)+b(u-v)=xu+yv$, then we solve the system of equations:
    \begin{numcases}{}
        a + b = x \tag*{(1)}\\
        a - b = y \tag*{(2)}
    \end{numcases}
    By adding (1) and (2), and using distributivity of multiplication over addition
    we have $\mathit{2}a = x + y$. Note that $\mathit{2}$ here is just a notation of $1+1$ in the field.
    \textbf{Since field is not of characteristic $2$, $\mathit{2}$ is not equal to $0$.}
    Then there exists a multiplicative inverse $\mathit{2}^{-1}$ in $F$.
    So by multiplying $\mathit{2}^-1$ on both side,
    we have $a = \frac{x+y}{\mathit{2}}$ and $b = \frac{x-y}{\mathit{2}}$.
    Thus we have
    \begin{align*}
        xu+yv & =a(u+v)+b(u-v)
              & =\frac{x+y}{\mathit{2}}(u+v)+\frac{x-y}{\mathit{2}} \\
    \end{align*}
    By the linear independence of $\set{u+v, u-v}$, we have
    \begin{numcases}{}
        \frac{x+y}{\mathit{2}} = 0 \tag*{(3)}\\
        \frac{x-y}{\mathit{2}} = 0 \tag*{(4)}
    \end{numcases}
    Similarly, thanks to field's characteristic is not equal to two,
    we can first multiply $\mathit{2}$ on both side of (3) and (4),
    then we can add (3) and (4) to get $x = 0$ and $y = 0$.
\end{proof}


\section{Note exercise 1.5.15}
\begin{quotation}
    15. Let $S = \set{u_1, u_2, \dots, u_n}$ be a finite set of vectors.
    Prove that $S$ is linearly dependent if and only if $u_1 = 0$ or
    $u_{k+1} \in \operatorname{span}(\set{u_1, u_2, \dots, u_k})$ for some
    $k$ such that $1 \leq k < n$.
\end{quotation}

The wording of the exercise is not so clear.
I think for the "$\implies$" part, it should be
\begin{quote}
    If $S = \set{u_1, u_2, \dots, u_n}$ is a linearly dependent set, then
    $\exists k$ satisfying $1 \leq k < n$ such that
    $u_{k+1} \in \operatorname{span}(\set{u_1, u_2, \dots, u_{k-1}})$.
\end{quote}
Note it's $\exists k$ rather than $\forall k$.

If it's $\forall k$, then the statement is not true. For example,
let $S$ be ${u_1,u_2,u_3}$, where
$u_1=(1,0)$,$u_2=(2,0)$ and $u_3=(0,1)$.
Clearly $S$ is linearly independent, but $u_3$ is not in the span of $\set{u_1,u_2}$.

Besides, I don't think the proof of "$\implies$" in the solution book is clear.

The following only shows the "$\implies$" part,
since the "$\impliedby$"
is simple and clearly showed in the solution book.

\begin{quotation}
    \textbf{Theorem 1.7.} Let $S$ be a linearly independent subset of a vector space
    $V$, and let $v$ be a vector in $V$ that is not in $S$. Then $S \cup \set{v}$
    is linearly dependent if and only if $v \in \operatorname{span}(S)$.
\end{quotation}

\begin{proof}
    Let induct on $n$.

    For $n=1$, we have $S=\set{u_1}$. If $S$ is linearly dependent, $u_1$ have to be $\0$.

    If the statement is true for $n$, we show it is also true for $n+1$.
    Since $\set{u_1,u_2,\dots,u_{n+1}}=\set{u_1,u_2,\dots,u_n}\cup\set{u_{n+1}}$
    If $\set{u_1,u_2,\dots,u_n}$ is linearly dependent, then by induction hypothesis,
    it's already true.
    If $\set{u_1,u_2,\dots,u_n}$ is linearly independent, then by Theorem 1.7, we get
    $u_n\in\operatorname{span}(\set{u_1,u_2,\dots,u_n})$

    Then finish the proof by induction.
\end{proof}

\section{Exercise 1.6.20}
Let $\mathsf{V}$ be a vector space having dimension $n$, and let $S$
be a subset of $\mathsf{V}$ that generates $\mathsf{V}$.

(a) Prove that there is a subset of $S$ that is a basis for $\mathsf{V}$.
(Be careful not to assume that $S$ is finite.)
(b)Prove that $S$ contains at least $n$ vectors.

\begin{quotation}
    \textbf{Theorem 1.9.} If a vector space $\mathsf{V}$ is generated by a finite set $S$, then
    some subset of $S$ is a basis for $\mathsf{V}$. Hence, $\mathsf{V}$
    has a finite basis.
\end{quotation}

Although there's solution in the anwser book, but I think there's a flaw, simce you
may not always able to stop the picking process in a infinite set.

The proof below is clipped from \href{https://math.stackexchange.com/a/4762926/808819}{StackExchange}

\begin{proof}
    (a)
    Take a basis  $\{e_1,\ldots,e_n\}$ of $V$.
    For each $k\in\{1,2,\ldots,n\}$, $e_k$ can be written as a linear
    combination of a finite subset $S_k$ of $S$. (Since $S$ spans $V$)
    Let $S^\ast=\bigcup_{k=1}^nS_k$.
    Then, since $\{e_1,\ldots,e_n\}$ spans $V$ and
    since each $e_k$ is a linear combination of elements of $S^\ast$,
    $S^\ast$ spans $V$. But $S^\ast$ is finite, and therefore it contains a basis of $V$.
    So, since $S^\ast\subset S$, $S$ contains a basis of $V$.

    (b) It is a direct consequence of (a) and theorem 1.9.
\end{proof}

\section{Note on exercise 1.6.21}
Prove that a vector space is infinite-dimensional if and only if it contains
an infinite linearly independent subset.

\begin{quotation}
    \textbf{Definition}
    A vector space $\mathsf{V}$ is called \textbf{finite-dimensional} if it has a basis
    consisting of a finite number of vectors. The unique number of vectors in each
    basis for $\mathsf{V}$ is called the \textbf{dimension} of $\mathsf{V}$ and is denoted
    by $\dim(\mathsf{V})$.

    A vector space that is not finite-dimensional is called \textbf{infinite-dimensional}.
\end{quotation}

The proof in the answer book is again not satisfying. Note in the textbook, there's only
definition of finite and infinite dimension vector space, but not showing that every vector
space has a basis, although the name of infinite dimension vector space implies that
there exists a basis for it. In my opinion, the proof should use the statement in section 1.7.



\section{Exercise 2.4.20}

\textbf{Exercise 2.20.} Let $T: \mathsf{V} \to \mathsf{W}$ be a linear transformation from an
$n$-dimensional vector space $\mathsf{V}$ to an $m$-dimensional vector space $\mathsf{W}$.
Let $\beta$ and $\gamma$ be ordered bases for $\mathsf{V}$ and $\mathsf{W}$, respectively.
Let $A = \left[ T \right]^\gamma_\beta$. Prove that
\(
\mathrm{rank}(T) = \mathrm{rank}(L_A)\)
and
\(
\mathrm{nullity}(T) = \mathrm{nullity}(L_A),
\)
where $L_A$ is the matrix transformation defined by $L_A(x) = Ax$.
\textit{Hint:} Apply Exercise 2.17 to Figure 2.2.

\begin{quotation}
    \textbf{Exercise 2.4.17.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces,
    and let $T: \mathsf{V} \to \mathsf{W}$ be an isomorphism. Let $\mathsf{V}_0$ be a subspace of $\mathsf{V}$.
    (a) Show that $T(\mathsf{V}_0)$ is a subspace of $\mathsf{W}$.
    (b) Show that $\dim(\mathsf{V}_0) = \dim(T(\mathsf{V}_0))$.

    \textbf{Theorem 2.14.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces
    having ordered bases $\beta$ and $\gamma$, respectively, and let
    $T: \mathsf{V} \to \mathsf{W}$ be linear.
    Then, for each $u \in \mathsf{V}$, we have
    \[
        \left[ T(u) \right]_\gamma = \left[ T \right]^\gamma_\beta \left[ u \right]_\beta.
    \]

    \begin{figure}[h]
        \centering
        \includegraphics[scale=1]{{Figure 2.2.pdf}}
        \captionsetup{labelformat=empty}
        \caption*{Figure 2.2}
    \end{figure}

    By a simple reformulation of Theorem 2.14 (p.~91), we may conclude that
    \[
        L_A  \phi_\beta = \phi_\gamma  T
    \]
\end{quotation}

\begin{proof}
    We use a lemma instead of hint to prove this.

    \begin{quotation}
        \textbf{Lemma.} Let T be a linear transformation and $\phi_1, \phi_2$ be isomorphisms,
        Then $\textrm{rank}(\phi_1T)=\textrm{rank}(T)$, $\textrm{rank}(T\phi_2)=\textrm{rank}(T)$ and
        $\textrm{nullity}(\phi_1T)=\textrm{nullity}(T)$, $\textrm{nullity}(T\phi_2)=\textrm{nullity}(T)$.

        \smallskip

        It's obvious for the rank part.
        For the nullity part,
        We have $\phi_1Tx$ is $\0$ iff $\phi_1x$ is in $N(T)$, since $\phi_1^{-1}$ is also an isomorphism,
        by conclusion of exercise 2.4.17, $\mathrm{dim}(\phi_1^{-1}(N(T)))=\mathrm{dim}(N(T))$. So
        $\mathrm{nullity}(\phi_1T)=\mathrm{nullity}(T)$.
        For $\textrm{nullity}(T\phi_2)=\textrm{nullity}(T)$, $T\phi_2x=\0$ iff $Tx=\0$, which impiles the conclusion.
    \end{quotation}

    As $L_A  \phi_\beta = \phi_\gamma  T$, exercise 2.4.20 is easily proved.
\end{proof}

\section{Exercise 2.5.13}
Although the solution book provides a proof, I want to note a more natural way to prove it.

\begin{quotation}
    \textbf{Exercise 2.5.13.} Let $\mathsf{V}$ be a finite-dimensional vector space over a field $F$, and let
    $\beta = \{x_1, x_2, \dots, x_n\}$ be an ordered basis for $\mathsf{V}$. Let $Q$ be an $n \times n$
    invertible matrix with entries from $F$. Define
    \[
        x'_j = \sum_{i=1}^{n} Q_{ij} x_i \quad \text{for } 1 \leq j \leq n,
    \]
    and set $\beta' = \{x'_1, x'_2, \dots, x'_n\}$.
    Then $\beta'$ is a basis for $\mathsf{V}$, and $Q$ is the change-of-coordinate matrix
    that converts $\beta'$-coordinates into $\beta$-coordinates.
\end{quotation}

\begin{proof}
    We consider matrix $Q$ to represent a linear transformation $T_Q$ from $\mathbb{F}^n$ to
    $\mathsf{V}$. Then $T_Q$ transforms each $e_j$ to $\sum_{i=1}^{n} Q_{ij} x_i=x_j'$
    respectively, where $e_j$ is the $j$-th standard basis vector in $\mathbb{F}^n$.

    Since $Q$ is invertible, $T_Q$ is an isomorphism, which means $\mathrm{rank}(T_Q) = n$.
    So $\sum_{i=1}^{n} Q_{ij} x_i=x_j'$ for $j$ from $1$ to $n$ is a basis for $\mathsf{V}$.

    Then the fact of $Q$ is change-of-coordinate matrix is obvious.
\end{proof}

\section{Exercise 2.6.20}
Let $\mathsf{V}$ and $\mathsf{W}$ be nonzero vector spaces over the same field,
and let $T: \mathsf{V} \to \mathsf{W}$ be a linear transformation.

(a) Prove that $T$ is onto if and only if $T^{\mathsf{t}}$ is one-to-one.

(b) Prove that $T^{\mathsf{t}}$ is onto if and only if $T$ is one-to-one.

\begin{proof}
    The proof is generated by GPT-5 Thinking, verified by myself.
    Let $V,W$ be nonzero vector spaces over the same field and $T:V\to W$ linear.
    The transpose (dual) map $T^{t}:W^{*}\to V^{*}$ is $T^{t}(\phi)=\phi\circ T$.

    (a) $T$ is onto  $\iff$  $T^{t}$ is one-to-one

    ($\implies$) If $T$ is onto and $T^{t}(\phi)=0$, then $\phi\circ T=0$.
    For any $w\in W$ there exists $v\in V$ with $T v=w$, hence
    $\phi(w)=\phi(Tv)=0$. Thus $\phi=0$, so $\mathsf{N}(T^{t})=\{\0\}$ and $T^{t}$ is injective.

    ($\impliedby$) If $T$ is not onto, then $\mathsf{R}(T)\not\subseteq W$ is a proper subspace.
    There exists a nonzero $\phi\in W^{*}$ with $\phi|_{\mathsf{R}(T)}=0$ (extend a basis of $\mathsf{R}(T)$ to one of $W$ and define $\phi$ to vanish on the first part). Then
    $T^{t}(\phi)=\phi\circ T=0$ while $\phi\neq 0$, so $T^{t}$ is not injective.

    (b) $T^{t}$ is onto  $\iff$  $T$ is one-to-one

    ($\implies$) Suppose $T^{t}$ is onto and pick $v\in\mathsf{N}T$.
    Choose $f\in V^{*}$ with $f(v)\neq 0$ (extend $v$ to a basis and take the dual).
    Surjectivity gives $\phi\in W^{*}$ with $T^{t}(\phi)=f$, i.e. $f=\phi\circ T$.
    Then $f(v)=\phi(Tv)=\phi(0)=0$, contradicting $f(v)\neq 0$. Hence $\mathsf{N}(T)=\{0\}$, so $T$ is injective.

    ($\impliedby$) If $T$ is injective, then $T:V\to \mathsf{R}(T)$ is an isomorphism.
    Given any $f\in V^{*}$, define $\psi:\mathsf{R}(T)\to \mathbb{F}$ by $\psi(Tv)=f(v)$; this is well-defined and linear.
    Extend $\psi$ to some $\phi\in W^{*}$ (extend a basis of $\mathsf{R}(T)$ to one of $W$).
    Then $T^{t}(\phi)=\phi\circ T=f$, showing $T^{t}$ is surjective.

    Thus,

    \[
        T\text{ onto } \iff T^{t}\text{ one-to-one}\quad\text{and}\quad T^{t}\text{ onto } \iff T\text{ one-to-one.}
    \]

\end{proof}


\newpage

\section{Takeaway}

\textbf{Definition.}
A vector space (or linear space) $\mathsf{V}$ over a field $F$ consists of
a set on which two operations (called addition and scalar multiplication,
respectively) are defined so that for each pair of elements $x, y$ in
$\mathsf{V}$ there is a unique element $x + y$ in $\mathsf{V}$, and for
each element $a$ in $F$ and each element $x$ in $\mathsf{V}$, there is a
unique element $ax$ in $\mathsf{V}$, such that the following conditions hold:

(VS1) For all $x, y \in \mathsf{V}$, $x + y = y + x$ (commutativity of addition).

(VS2) For all $x, y, z \in \mathsf{V}$, $(x + y) + z = x + (y + z)$
(associativity of addition).

(VS3) There exists an element in $\mathsf{V}$ denoted by $\0$ such that
$x + \0 = x$ for each $x \in \mathsf{V}$.

(VS4) For each element $x \in \mathsf{V}$, there exists an element
$y \in \mathsf{V}$ such that $x + y = \0$.

(VS5) For each element $x \in \mathsf{V}$, $1x = x$.

(VS6) For each pair of elements $a, b \in F$ and each element
$x \in \mathsf{V}$, $(ab)x = a(bx)$.

(VS7) For each element $a \in F$ and each pair of elements
$x, y \in \mathsf{V}$, $a(x + y) = ax + ay$.

(VS8) For each pair of elements $a, b \in F$ and each element
$x \in \mathsf{V}$, $(a + b)x = ax + bx$.

The elements $x + y$ and $ax$ are called the sum of $x$ and $y$ and the
product of $a$ and $x$, respectively.

\medskip

\textbf{Theorem 1.3.}
Let $\mathsf{V}$ be a vector space and $W$ a subset of
$\mathsf{V}$. Then $W$ is a subspace of $\mathsf{V}$ if and
only if the following three conditions hold for the operations
defined in $\mathsf{V}$:

(a) $\0 \in W$.

(b) $x + y \in W$ whenever $x \in W$ and $y \in W$.

(c) $c x \in W$ whenever $c \in F$ and $x \in W$.

\medskip

\textbf{Definition.}
Let $\mathsf{V}$ be a vector space and $S$ a nonempty subset of
$\mathsf{V}$. A vector $v \in \mathsf{V}$ is called a \textbf{linear combination}
of vectors of $\S$ if there exist a finite number of vectors
$u_1, u_2, \dots, u_n$ in $S$ and scalars $a_1, a_2, \dots, a_n$
in $F$ such that
\[
    v = a_1 u_1 + a_2 u_2 + \dots + a_n u_n.
\]
In this case, we also say that $v$ is a linear combination of
$u_1, u_2, \dots, u_n$ and call $a_1, a_2, \dots, a_n$ the \textbf{coefficients}
of the linear combination.

\medskip

\textbf{Definition.}
A subset $S$ of a vector space $\mathsf{V}$ is called \textbf{linearly
    dependent} if there exist a finite number of distinct vectors
$u_1, u_2, \dots, u_n$ in $S$ and scalars $a_1, a_2, \dots, a_n$,
not all zero, such that
\[
    a_1 u_1 + a_2 u_2 + \dots + a_n u_n = \0.
\]
In this case, we also say that the vectors of $S$ are linearly dependent.

\medskip

\textbf{Definition.}
A subset $S$ of a vector space $\mathsf{V}$ that is not linearly
dependent is called \textbf{linearly independent}.

As before, we also say that the vectors of $S$ are linearly independent.

\medskip

\textbf{Definition.}
A \textbf{basis} $\beta$ for a vector space $\mathsf{V}$ is a linearly independent
subset of $\mathsf{V}$ that generates $\mathsf{V}$.

If $\beta$ is a basis for $\mathsf{V}$, we also say that the vectors of
$\beta$ form a basis for $\mathsf{V}$.

\medskip

\textbf{Theorem 1.8.} Let $\mathsf{V}$ be a vector space and
$\beta = \{ u_1, u_2, \dots, u_n \}$ be a subset of $\mathsf{V}$.
Then $\beta$ is a basis for $\mathsf{V}$ if and only if each
$v \in \mathsf{V}$ can be uniquely expressed as a linear
combination of vectors of $\beta$, that is, can be expressed in the form
\[
    v = a_1 u_1 + a_2 u_2 + \dots + a_n u_n
\]
for unique scalars $a_1, a_2, \dots, a_n$.

\medskip

\textbf{Theorem 1.9.} If a vector space $\mathsf{V}$ is generated by a finite
set $S$, then some subset of $S$ is a basis for $\mathsf{V}$.
Hence, $\mathsf{V}$ has a finite basis.

\medskip

\textbf{Theorem 1.10 (Replacement Theorem).}
Let $\mathsf{V}$ be a vector space that is generated by a set $G$ containing
exactly $n$ vectors, and let $L$ be a linearly independent subset of
$\mathsf{V}$ containing exactly $m$ vectors. Then $m \leq n$ and there
exists a subset $H$ of $G$ containing exactly $n - m$ vectors such that
$L \cup H$ generates $\mathsf{V}$.

\medskip

\textbf{Corollary 1 of theorem 1.10.} Let $\mathsf{V}$ be a vector space having a finite basis.
Then every basis for $\mathsf{V}$ contains the same number of vectors.

\medskip

\textbf{Definition}
A vector space $\mathsf{V}$ is called \textbf{finite-dimensional} if it has a basis
consisting of a finite number of vectors. The unique number of vectors in each
basis for $\mathsf{V}$ is called the \textbf{dimension} of $\mathsf{V}$ and is denoted
by $\dim(\mathsf{V})$.

A vector space that is not finite-dimensional is called \textbf{infinite-dimensional}.

\medskip

\textbf{Corollary 2 of theorem 1.10.} Let $\mathsf{V}$ be a vector space with dimension $n$.

(a) Any finite generating set for $\mathsf{V}$ contains at least $n$ vectors,
and a generating set for $\mathsf{V}$ that contains exactly $n$ vectors
is a basis for $\mathsf{V}$.

(b) Any linearly independent subset of $\mathsf{V}$ that contains exactly
$n$ vectors is a basis for $\mathsf{V}$.

(c) Every linearly independent subset of $\mathsf{V}$ can be extended to
a basis for $\mathsf{V}$.

\medskip

\textbf{Definition.}
Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces over $F$. We call a function
$T: \mathsf{V} \to \mathsf{W}$ a \textbf{linear transformation from $\mathsf{V}$ to $\mathsf{W}$} if,
for all $x, y \in \mathsf{V}$ and $c \in F$, we have:

(1) $T(x + y) = T(x) + T(y)$

(2) $T(cx) = cT(x)$

\medskip

\textbf{Definition.} For vector spaces $\mathsf{V}$ and $\mathsf{W}$ over $F$, we define the
\textbf{identity transformation} $\mathsf{I_V}: \mathsf{V} \to \mathsf{V}$
by $\mathsf{I_V}(x) = x$ for all $x \in \mathsf{V}$,
and the \textbf{zero transformation} $\mathsf{T_0}: \mathsf{V} \to \mathsf{W}$
by $\mathsf{T_0}(x) = \0$ for all $x \in \mathsf{V}$.

\textsl{It is clear that both of these transformations are linear.}

\medskip

\textbf{Definitions.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces, and let
$T: \mathsf{V} \to \mathsf{W}$ be linear.

We define the \textbf{null space} (or \textbf{kernel}) $\mathsf{N}(T)$ of $T$ to be the set of all vectors
$x$ in $\mathsf{V}$ such that $T(x) = \0$; that is,
$\mathsf{N}(T) = \set{x \in \mathsf{V} \mid T(x) = \0}$.

We define the \textbf{range} (or \textbf{image}) $\mathsf{R}(T)$ of $T$ to be the subset of $\mathsf{W}$
consisting of all images (under $T$) of vectors in $\mathsf{V}$; that is,
$\mathsf{R}(T) = \set{T(x) \mid x \in \mathsf{V}}$.

\medskip

\textbf{Theorem 2.1.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces, and let
$T: \mathsf{V} \to \mathsf{W}$ be linear.

Then $\mathsf{N}(T)$ and $\mathsf{R}(T)$ are subspaces of $\mathsf{V}$ and $\mathsf{W}$, respectively.

\medskip

\textbf{Theorem 2.2.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces, and let
$T: \mathsf{V} \to \mathsf{W}$ be linear.

If $\beta = \set{v_1, v_2, \dots, v_n}$ is a basis for $\mathsf{V}$, then
\[
    \mathsf{R}(T) = \mathrm{span}(T(\beta)) = \mathrm{span}(\set{T(v_1), T(v_2), \dots, T(v_n)}).
\]

\medskip

\textbf{Definitions.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces, and let
$T: \mathsf{V} \to \mathsf{W}$ be linear.
If $\mathsf{N}(T)$ and $\mathsf{R}(T)$ are finite-dimensional, then we define the
\textbf{nullity} of $T$, denoted $\mathrm{nullity}(T)$, and the \textbf{rank} of $T$,
denoted $\mathrm{rank}(T)$, to be the dimensions of $\mathsf{N}(T)$ and $\mathsf{R}(T)$, respectively.

\medskip

\textbf{Theorem 2.3 (Dimension Theorem).} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces,
and let $T: \mathsf{V} \to \mathsf{W}$ be linear.
If $\mathsf{V}$ is finite-dimensional, then
\[
    \mathrm{nullity}(T) + \mathrm{rank}(T) = \dim(\mathsf{V}).
\]

\medskip

\textbf{Theorem 2.4.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces, and let
$T: \mathsf{V} \to \mathsf{W}$ be linear.
Then $T$ is one-to-one if and only if $\mathsf{N}(T) = \set{\0}$.

\medskip

\textbf{Theorem 2.5.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces of equal (finite)
dimension, and let $T: \mathsf{V} \to \mathsf{W}$ be linear. Then the following are equivalent:

(a) $T$ is one-to-one.

(b) $T$ is onto.

(c) $\mathrm{rank}(T) = \dim(\mathsf{V})$.

\medskip

\textbf{Theorem 2.6.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces over $F$, and suppose that
$\set{v_1, v_2, \dots, v_n}$ is a basis for $\mathsf{V}$.
For $w_1, w_2, \dots, w_n$ in $\mathsf{W}$, there exists exactly one linear transformation
$T: \mathsf{V} \to \mathsf{W}$ such that $T(v_i) = w_i$ for $i = 1, 2, \dots, n$.

\medskip

\textbf{Corollary.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces, and suppose that
$\mathsf{V}$ has a finite basis $\set{v_1, v_2, \dots, v_n}$.
If $U, T: \mathsf{V} \to \mathsf{W}$ are linear and $U(v_i) = T(v_i)$ for
$i = 1, 2, \dots, n$, then $U = T$.

\medskip

For the vector space $F^n$, we call $\{e_1, e_2, \dots, e_n\}$ the
\textbf{standard ordered basis} for $F^n$. Similarly, for the vector space $\mathsf{P}_n(F)$,
we call $\{1, x, \dots, x^n\}$ the \textbf{standard ordered basis} for $\mathsf{P}_n(F)$.

\medskip

\textbf{Theorem 2.7.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces over a field $F$,
and let $T, U: \mathsf{V} \to \mathsf{W}$ be linear.

(a) For all $a \in F$, $aT + U$ is linear.

(b) Using the operations of addition and scalar multiplication in the preceding
definition, the collection of all linear transformations from $\mathsf{V}$ to
$\mathsf{W}$ is a vector space over $F$.

\medskip

\textbf{Theorem 2.8.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces
with ordered bases $\beta$ and $\gamma$, respectively, and let $T, U: \mathsf{V} \to \mathsf{W}$
be linear transformations. Then

(a) $\left[ T + U \right]^\gamma_\beta = \left[ T \right]^\gamma_\beta + \left[ U \right]^\gamma_\beta$

(b) $\left[ aT \right]^\gamma_\beta = a \left[ T \right]^\gamma_\beta$ for all scalars $a$.

\medskip

\textbf{Theorem 2.11.} Let $\mathsf{V}, \mathsf{W},$ and $\mathsf{Z}$ be finite-dimensional vector spaces
with ordered bases $\alpha, \beta,$ and $\gamma$, respectively. Let
$T: \mathsf{V} \to \mathsf{W}$ and $U: \mathsf{W} \to \mathsf{Z}$ be linear transformations.
Then
\[
    \left[ UT \right]^\gamma_\alpha = \left[ U \right]^\gamma_\beta \left[ T \right]^\beta_\alpha.
\]

\medskip

\textbf{Theorem 2.14.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces
having ordered bases $\beta$ and $\gamma$, respectively, and let
$T: \mathsf{V} \to \mathsf{W}$ be linear.
Then, for each $u \in \mathsf{V}$, we have
\[
    \left[ T(u) \right]_\gamma = \left[ T \right]^\gamma_\beta \left[ u \right]_\beta.
\]

\medskip

\textbf{Theorem 2.17.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces, and let
$T: \mathsf{V} \to \mathsf{W}$ be linear and invertible.
Then $T^{-1}: \mathsf{W} \to \mathsf{V}$ is linear.

\medskip

\textbf{Theorem 2.18.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces
with ordered bases $\beta$ and $\gamma$, respectively. Let
$T: \mathsf{V} \to \mathsf{W}$ be linear.
Then $T$ is invertible if and only if $\left[ T \right]^\gamma_\beta$ is invertible.
Furthermore,
\(
\left[ T^{-1} \right]^\beta_\gamma = \left( \left[ T \right]^\gamma_\beta \right)^{-1}.
\)

\medskip

\textbf{Definitions.} Let $\mathsf{V}$ and $\mathsf{W}$ be vector spaces.
We say that $\mathsf{V}$ is \textbf{isomorphic} to $\mathsf{W}$ if there exists a linear transformation
$T: \mathsf{V} \to \mathsf{W}$ that is invertible. Such a linear transformation is called an
\textbf{isomorphism} from $\mathsf{V}$ onto $\mathsf{W}$.

\medskip

\textbf{Theorem 2.19.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces
over the same field. Then $\mathsf{V}$ is isomorphic to $\mathsf{W}$ if and only if
\(
\dim(\mathsf{V}) = \dim(\mathsf{W}).
\)

\medskip

\textbf{Theorem 2.20.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces
over a field $F$ of dimensions $n$ and $m$, respectively, and let $\beta$ and $\gamma$
be ordered bases for $\mathsf{V}$ and $\mathsf{W}$, respectively.

Then the function
\(
\Phi: \mathcal{L}(\mathsf{V}, \mathsf{W}) \to \mathsf{M}_{m \times n}(F)
\)
defined by
\(
\Phi(T) = \left[ T \right]^\gamma_\beta \quad \text{for } T \in \mathcal{L}(\mathsf{V}, \mathsf{W})
\)
is an isomorphism.

\medskip

\textbf{Theorem 2.22.} Let $\beta$ and $\beta'$ be two ordered bases for a finite-dimensional
vector space $\mathsf{V}$, and let $Q = \left[ I_{\mathsf{V}} \right]^\beta_{\beta'}$. Then:

(a) $Q$ is invertible.

(b) For any $v \in \mathsf{V}$,
\(
\left[ v \right]_\beta = Q \left[ v \right]_{\beta'}.
\)

\medskip

\textbf{Theorem 2.23.} Let $T$ be a linear operator on a finite-dimensional vector
space $\mathsf{V}$, and let $\beta$ and $\beta'$ be ordered bases for $\mathsf{V}$.

Suppose that $Q$ is the change-of-coordinate matrix that converts
$\beta'$-coordinates into $\beta$-coordinates. Then
\[
    \left[ T \right]_{\beta'} = Q^{-1} \left[ T \right]_{\beta} Q.
\]


\medskip

\textbf{Definition.} For a vector space $\mathsf{V}$ over a field $F$, we define the
\textbf{dual space} of $\mathsf{V}$ to be the vector space $\mathcal{L}(\mathsf{V}, F)$,
denoted by $\mathsf{V}^*$.

\medskip

\textbf{Theorem 2.24.} Suppose that $\mathsf{V}$ is a finite-dimensional vector space with
ordered basis $\beta = \{x_1, x_2, \dots, x_n\}$. Let $\mathsf{f}_i$ (for $1 \leq i \leq n$) be the
$i$th coordinate function with respect to $\beta$ as just defined, and let
$\beta^* = \{\mathsf{f}_1, \mathsf{f}_2, \dots, \mathsf{f}_n\}$.
Then $\beta^*$ is an ordered basis for the dual space $\mathsf{V}^*$, and for any
$\mathsf{f} \in \mathsf{V}^*$, we have
\[
    \mathsf{f} = \sum_{i=1}^{n} \mathsf{f}(x_i)\mathsf{f}_i.
\]

\medskip

\textbf{Definition.} Using the notation of Theorem 2.24, we call the ordered basis
$\beta^* = \{\mathsf{f}_1, \mathsf{f}_2, \dots, \mathsf{f}_n\}$ of the dual space $\mathsf{V}^*$ that satisfies
$\mathsf{f}_i(x_j) = \delta_{ij}$ for $1 \leq i, j \leq n$ the \textbf{dual basis} of $\beta$.

\medskip

\textbf{Theorem 2.25.} Let $\mathsf{V}$ and $\mathsf{W}$ be finite-dimensional vector spaces over
$F$ with ordered bases $\beta$ and $\gamma$, respectively. For any linear transformation
$T: \mathsf{V} \to \mathsf{W}$, the mapping
$T^{\mathsf{t}} : \mathsf{W}^* \to \mathsf{V}^*$
defined by $T^{\mathsf{t}}(\mathsf{g}) = \mathsf{g}T$ for all $\mathsf{g} \in \mathsf{W}^*$
is a linear transformation with the property that
\(
\left[ T^{\mathsf{t}} \right]^{\beta^*}_{\gamma^*} = \left( \left[ T \right]^\gamma_\beta \right)^{\mathsf{t}}.
\)

\textbf{Definition.} (Exponential rule) Let $c = a + ib$ be a complex number with real part $a$ and
imaginary part $b$. Define
\[
    e^c = e^a(\cos b + i \sin b).
\]
The special case
\[
    e^{ib} = \cos b + i \sin b
\]
is called \textbf{Euler's formula}.

\textbf{Theorem 2.28.} The set of all solutions to a homogeneous linear differential equation
with constant coefficients coincides with the null space of $\mathsf{p}(\mathsf{D})$,
where $\mathsf{p}(t)$ is the auxiliary polynomial associated with the equation.

\textbf{Corollary.} For any complex number $c$, the null space of the differential operator
$\mathsf{D} - c\mathsf{I}$ has $\{e^{ct}\}$ as a basis.

\textbf{Theorem 2.32.} For any differential operator $p(\mathsf{D})$ of order $n$,
the null space of $p(\mathsf{D})$ is an $n$-dimensional subspace of $C^\infty$.

\textbf{Theorem 2.33.} Given $n$ distinct complex numbers $c_1, c_2, \dots, c_n$,
the set of exponential functions $\{e^{c_1 t}, e^{c_2 t}, \dots, e^{c_n t}\}$ is linearly independent.

\textbf{Theorem 2.34.} Given a homogeneous linear differential equation with
constant coefficients and auxiliary polynomial
\[
    (t - c_1)^{n_1}(t - c_2)^{n_2} \cdots (t - c_k)^{n_k},
\]
where $n_1, n_2, \dots, n_k$ are positive integers and $c_1, c_2, \dots, c_k$ are distinct
complex numbers, the following set is a basis for the solution space of the equation:
\[
    \{e^{c_1 t},\ te^{c_1 t},\ \dots,\ t^{n_1 - 1}e^{c_1 t},\ \dots,\ e^{c_k t},\ te^{c_k t},\ \dots,\ t^{n_k - 1}e^{c_k t}\}.
\]

\end{document}